import pandas as pd
import numpy as np
import re
import json
from typing import TypedDict, List
from langgraph.graph import END, StateGraph
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os

# Load environment variables from .env
load_dotenv()

# Initialize the LLM
groq_api = os.getenv("groq_api_key")
llm = ChatGroq(
    groq_api_key=groq_api,
    temperature=0.3,
    model_name="gemma2-9b-it",
)

# Data Loading and Document Preparation
df = pd.read_csv("./content/defects.csv")
try:
    test_cases_df = pd.read_csv("./content/test_cases.csv")
except Exception as e:
    test_cases_df = pd.DataFrame(columns=["Module", "Test_Scenario", "Test_Steps", "Pre_Requisite", "Pass_Fail_Criteria", "Expected_Result"])

docs = []
for _, row in df.iterrows():
    if pd.notna(row["Description"]) and pd.notna(row["Solution"]):
        docs.append(Document(
            page_content=row["Description"],
            metadata={
                "solution": row["Solution"],
                "module": row["Module"]
            }
        ))

# Create embeddings and FAISS vector store for retrieval
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(search_kwargs={"k": 1})

# Similarity Functions
def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))

def get_embedding(text: str) -> np.ndarray:
    return np.array(embeddings.embed_query(text))

def is_similar(query: str, document: Document, threshold: float = 0.5) -> bool:
    query_emb = get_embedding(query)
    doc_emb = get_embedding(document.page_content)
    sim = cosine_similarity(query_emb, doc_emb)
    return sim >= threshold

# Helper: Retrieve CSV Test Cases by Module
def get_csv_test_cases(module: str) -> List[dict]:
    valid_cases = []
    global test_cases_df
    if test_cases_df.empty:
        return valid_cases
    df_filtered = test_cases_df[test_cases_df["Module"] == module]
    required_fields = ["Test_Scenario", "Test_Steps", "Pre_Requisite", "Pass_Fail_Criteria", "Expected_Result"]
    for _, row in df_filtered.iterrows():
        if all(str(row.get(field, "")).strip() for field in required_fields):
            tc_dict = {field: row[field] for field in required_fields}
            tc_dict["Module"] = row["Module"]
            valid_cases.append(tc_dict)
    return valid_cases

# Helper: Save New Test Cases to CSV (avoiding duplicates)
def save_new_test_cases(new_cases: List[dict]):
    global test_cases_df
    required_columns = ["Module", "Test_Scenario", "Test_Steps", "Pre_Requisite", "Pass_Fail_Criteria", "Expected_Result"]
    if test_cases_df.empty:
        test_cases_df = pd.DataFrame(columns=required_columns)
    rows_to_add = []
    for case in new_cases:
        duplicate = test_cases_df[
            (test_cases_df["Module"] == case["Module"]) &
            (test_cases_df["Test_Scenario"] == case["Test_Scenario"]) &
            (test_cases_df["Test_Steps"] == case["Test_Steps"]) &
            (test_cases_df["Pre_Requisite"] == case["Pre_Requisite"]) &
            (test_cases_df["Pass_Fail_Criteria"] == case["Pass_Fail_Criteria"]) &
            (test_cases_df["Expected_Result"] == case["Expected_Result"])
        ]
        if duplicate.empty:
            rows_to_add.append(case)
    if rows_to_add:
        new_df = pd.DataFrame(rows_to_add)
        test_cases_df = pd.concat([test_cases_df, new_df], ignore_index=True)
        test_cases_df.to_csv("./content/test_cases.csv", index=False)

# Define Agent State and Workflow Node for Test Case Generation
class AgentState(TypedDict):
    input: str
    context: List[Document]
    response: dict

def validate_or_generate_test_cases(state: AgentState):
    try:
        if not state["context"]:
            return {"response": {"Error": "**Error**: The defect could not be found in the database."}}
        context = state["context"][0]
        error_message = state["input"]

        # Validate similarity between input and defect description
        if not is_similar(error_message, context, threshold=0.6):
            return {"response": {"Error": "**Error**: The defect could not be found in the database."}}

        solution = context.metadata["solution"]
        module = context.metadata["module"]

        # Generate explanation for why the solution works
        explanation_prompt = """
        [INST] Explain why this solution fixes the following error:
        Error: {error}
        Solution: {solution}
        [/INST]
        """
        explanation_template = ChatPromptTemplate.from_template(explanation_prompt)
        formatted_explanation = explanation_template.format_prompt(error=error_message, solution=solution)
        explanation = llm.invoke(formatted_explanation.to_messages()).content.strip()

        # Retrieve test cases from CSV for the module
        csv_test_cases = get_csv_test_cases(module)
        REQUIRED_TEST_CASE_COUNT = 4

        if csv_test_cases and len(csv_test_cases) >= REQUIRED_TEST_CASE_COUNT:
            selected_cases = csv_test_cases
        elif csv_test_cases and len(csv_test_cases) < REQUIRED_TEST_CASE_COUNT:
            num_to_generate = REQUIRED_TEST_CASE_COUNT - len(csv_test_cases)
            additional_prompt = """
            [INST] Generate {num} additional comprehensive test case(s) to fully validate the following defect solution end-to-end in JSON format.
            Error: {error}
            Solution: {solution}
            Explanation: {explanation}
            Ensure that these test cases do not duplicate the following existing test cases:
            {existing_test_cases}
            Each test case MUST have the following keys: "Test_Scenario", "Test_Steps", "Pre_Requisite", "Expected_Result", "Pass_Fail_Criteria".
            Return the output as a JSON array.
            [/INST]
            """
            existing_str = json.dumps(csv_test_cases, indent=2)
            additional_template = ChatPromptTemplate.from_template(additional_prompt)
            formatted_additional = additional_template.format_prompt(
                num=num_to_generate,
                error=error_message,
                solution=solution,
                explanation=explanation,
                existing_test_cases=existing_str
            )
            additional_response = llm.invoke(formatted_additional.to_messages()).content.strip()
            try:
                additional_cases = json.loads(additional_response)
            except Exception as e:
                additional_cases = []
            selected_cases = csv_test_cases + additional_cases
            if additional_cases:
                save_new_test_cases(additional_cases)
        else:
            full_prompt = """
            [INST] Generate a comprehensive set of test cases to fully validate the following defect solution end-to-end in JSON format.
            Error: {error}
            Solution: {solution}
            Explanation: {explanation}
            Each test case MUST have the following keys: "Test_Scenario", "Test_Steps", "Pre_Requisite", "Expected_Result", "Pass_Fail_Criteria".
            Return the output as a JSON array.
            [/INST]
            """
            full_template = ChatPromptTemplate.from_template(full_prompt)
            formatted_full = full_template.format_prompt(error=error_message, solution=solution, explanation=explanation)
            full_response = llm.invoke(formatted_full.to_messages()).content.strip()
            try:
                selected_cases = json.loads(full_response)
            except Exception as e:
                selected_cases = []

            if selected_cases:
                save_new_test_cases(selected_cases)

        result = {
            "Error": error_message,
            "Solution": solution,
            "Explanation": explanation,
            "TestCases": selected_cases
        }
        return {"response": result}
    except Exception as e:
        return {"response": {"Error": f"Error processing request: {str(e)}"}}

def auto_evaluate_solution(response: dict) -> int:
    if isinstance(response, dict) and "TestCases" in response and isinstance(response["TestCases"], list) and len(response["TestCases"]) > 0:
         return 5
    else:
         return 1

def generate_alternative_solution(error_message: str) -> dict:
    alt_prompt = """
    [INST] Provide a concise, actionable alternative solution for the following error:
    Error: {error}
    Ensure that the solution is clear and does not include any follow-up questions.
    [/INST]
    """
    alt_template = ChatPromptTemplate.from_template(alt_prompt)
    formatted_alt = alt_template.format_prompt(error=error_message)
    alternative_solution = llm.invoke(formatted_alt.to_messages()).content.strip()

    test_case_prompt = """
    [INST] Given the error and the alternative solution:
    Error: {error}
    Solution: {solution}
    Generate EXACTLY 4 structured test cases in JSON format. Each test case must have the following keys: "Test_Scenario", "Test_Steps", "Pre_Requisite", "Expected_Result", "Pass_Fail_Criteria".
    Return the output as a JSON array.
    [/INST]
    """
    tc_template = ChatPromptTemplate.from_template(test_case_prompt)
    formatted_tc = tc_template.format_prompt(error=error_message, solution=alternative_solution)
    alternative_test_cases_output = llm.invoke(formatted_tc.to_messages()).content.strip()
    try:
        alternative_test_cases = json.loads(alternative_test_cases_output)
    except Exception as e:
        alternative_test_cases = []
    return {
        "AlternativeSolution": alternative_solution,
        "TestCases": alternative_test_cases
    }

def get_solution_autonomously(error_message: str) -> dict:
    max_iterations = 3
    iteration = 0
    while iteration < max_iterations:
        result = agent.invoke({"input": error_message.strip()})
        response = result["response"]
        rating = auto_evaluate_solution(response)
        if rating < 3:
            alt_response = generate_alternative_solution(error_message)
            return alt_response
        else:
            return response
        iteration += 1
    return response

# Build the State Graph Workflow and compile the agent
workflow = StateGraph(AgentState)
workflow.add_node("retrieve", lambda state: {"context": retriever.invoke(state["input"])})
workflow.add_node("validate_or_generate_test_cases", validate_or_generate_test_cases)
workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "validate_or_generate_test_cases")
workflow.add_edge("validate_or_generate_test_cases", END)
agent = workflow.compile()

def get_solution(error_message: str) -> dict:
    """
    Given an error/defect description, returns a dictionary containing:
      - Error: The input error/defect.
      - Solution: The proposed solution.
      - Explanation: Why the solution works.
      - TestCases: A list of test case dictionaries.
    """
    return get_solution_autonomously(error_message)

if __name__ == "__main__":
    # For standalone testing
    error_description = "BIOS not booting up"
    final_solution = get_solution(error_description)
    print(json.dumps(final_solution, indent=2))






import streamlit as st
import pandas as pd
from agent import get_solution

st.title("Defect RCA Assistant")
st.write("Enter an error or defect description below to generate a structured RCA report with test cases.")

error_input = st.text_area("Error/Defect Description", height=100)

if st.button("Get RCA Report"):
    if not error_input.strip():
        st.error("Please enter an error or defect description.")
    else:
        with st.spinner("Processing..."):
            result = get_solution(error_input)
        
        # Display error, solution and explanation as markdown
        if "Error" in result:
            st.markdown(f"**Error:** {result['Error']}")
        if "Solution" in result:
            st.markdown(f"**Solution:** {result['Solution']}")
        if "Explanation" in result:
            st.markdown(f"**Explanation:** {result['Explanation']}")
        
        # Display the test cases in tabular form if available
        test_cases = result.get("TestCases", [])
        if test_cases and isinstance(test_cases, list):
            st.subheader("Generated Test Cases")
            df = pd.DataFrame(test_cases)
            if not df.empty:
                st.table(df)
            else:
                st.write("No test cases were generated.")
        else:
            st.write("No test cases available.")
